{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1 - Implementing and training a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment verification\n",
    "Start by confirming you have PyTorch, TorchVision and TensorBoard installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T23:44:27.402507541Z",
     "start_time": "2023-10-10T23:44:24.157569130Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## QUESTIONS - General autonomous driving questions\n",
    "In this part, some general questions about autonomous driving, both general and specific to formula student, are presented. You should read the relevant parts of the rulebook and beginner's guide to answer some of the questions (they are attached in /docs folder of the repo). Feel free to use the internet.\n",
    "\n",
    "1. List some pros and cons of using a stereo camera versus LiDAR versus RADAR for perception. You can research examples from the industry on why do they use specific sensors and not others. \n",
    "2. Stereo cameras are capable of perceiving both color and depth for each pixel. These cameras can be bought plug-and-play solutions (for example Intel RealSense or StereoLabs ZED 2) or self-made using industrial cameras (for example Basler). Computing depth from multiple cameras requires processing, called \"depth estimation\", which is done onboard on the plug and play solutions. Which solution would you opt for if you had a small team with a short budget? Consider complexity, reliability and cost on your decision.\n",
    "3. In an autonomous car, monitorization and reaction to critical failures are essential to prevent uncontrolled behavior. According to the rulebook and the beginner's guide, what must happen if the car detects a camera and/or LiDAR malfunction? Select the correct option(s), mentioning the relevant rule(s) you found:\n",
    "    1. Play a sound using the TSAC.\n",
    "    2. Eject the processing computer.\n",
    "    3. Activate the EBS.\n",
    "    4. Send a text message to the officials notifying the issue.\n",
    "    5. Autonomously approach the ASR to perform a safe shutdown.\n",
    "4. Usually an autonomous driving pipeline is divided into perception, planning and control. Which algorithms are most commonly used by formula student teams on each of these stages? You can research other teams' social media or FSG Academy, for example.\n",
    "5. On a Formula Student car with an Autonomous System, for the car to operate on Manual Mode (with a driver) some verifications must be done by the Autonomous System. What are they?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. \n",
    "It all depends on the context, obviously. A stereo camera, let’s say an RGBD one that can capture both image and depth, can be super useful, for example, if we’re training models that need to recognize some kinda of objects present on the image, having that depth perception is a huge plus. You can even find some solutions for that with pre-processing that’ll give you a thermal image with depth info on the different elements (https://github.com/apple/ml-depth-pro). But honestly, having that real-time depth perception right from the start is a big advantage.\n",
    "If you’ve got something moving—like a drone or a car—depth perception is a must for making quick decisions. And getting depth directly from the camera saves a lot of resources, especially compared to a 2D camera, where you’d have to do way more processing to get the same kind of data, like i said before.\n",
    "\n",
    "Now, lidar, on the other hand, is awesome when you just need to know distances and mapping the environment without worrying about any visual images. So, if we don’t think image capture is critical, we can just use lidar for measuring distances, depending on the type of robot. Like a robot vacuum, for example. Those don’t need a camera; just lidar and a few sensors to map out their surroundings are more than enough to do the job.\n",
    "\n",
    "And about radar, it works by sending out radio waves and catching the reflection to calculate distances and object speeds. So Radar’s also better at precisely measuring speed, which is why it’s used in cars to detect other vehicles on the road and gauge their approach accurately. In a nutshell, if you need a system that performs well in all kinds of lighting or weather conditions, radar’s looks like a solid choice at least."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\n",
    "Honestly, if I’ve got a small team and a tight budget, I’d probably go with a plug-and-play solution. With these, you get the depth estimation already built-in, which means I don’t have to worry about setting up a bunch of extra processing on our end. Less hassle, fewer headaches.\n",
    "\n",
    "Building a custom setup is cool and can be more customizable, sure, but it’s also way more complex. You’d need to handle all that depth estimation yourself, which can be a time suck, not to mention it requires people who really know what they’re doing. And if you’re on a short budget, the extra resources needed for a custom build can quickly add up.\n",
    "\n",
    "What i usually like to approach is something like \"Will need to use this again?\" like on other projects or so, if i known for certain that i will, so i start searching the ways to customize the camera my self in my free time, off course it will take a while but in a long run it will be more money saved, and with more knowledge in the topic probably the better it will end up.\n",
    "\n",
    "So, in terms of reliability and cost-effectiveness, plug-and-play wins here.  But never let end solutions be ALWAYS the first choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. \n",
    "Activate the EBS! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\n",
    "For perception, teams looks to often use LiDAR and a camera data for cone detection and rely on sensor fusion to increase reliability. SLAM (Simultaneous Localization and Mapping) algorithms like GraphSLAM (what is kinda wild since to make it work in real time shouldn't be that easy) are common for mapping the track and localizing the car.\n",
    "\n",
    "In the planning stage, the path finding is handled with algorithms like A* (or A*-based methods) or RRT (Rapidly-exploring Random Tree). Model Predictive Control (MPC) is widely used for trajectory optimization like adjusting the future moves (at this point im starting to realize that maybe this computationally intensive solutions cannot be replaced... thats may be the reason that teslas have a good amount of teraflops).\n",
    "\n",
    "For control, teams use lateral control methods like Pure Pursuit or the Stanley Controller to keep the car on track (that i honestly have no idea how they work). For longitudinal control, PID controllers manage speed by adjusting throttle and braking (that when searching a little bit more about it, i founded that he can do a lot of things like temperature, flow, pressure, speed, that im not sure that they are using or not! but they should be using it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\n",
    "Before it can be driven in a manual model, the Autonomous System Master Switch (ASMS) need to be switched to off, Autonomous System Brake (ASB) needs to be completely inactive (we dont want it to brake by it self xD (maybe thats not a bad idea actually having like a level of permission that it can have on the car, just to serve as a secure measurement)) and the Emergency Brake System (EBS) should stay off too, resuming, the pilot do the braking. The driver should activate the Tractive System (TS) to! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Dataset\n",
    "The used dataset is the well-known MNIST, which is composed of images of handwritten digits (0 to 9) with 28 pixels wide and 28 pixels high.\n",
    "\n",
    "The goals of most of the models using this dataset is to classify the digit of the image, which is our case.\n",
    "\n",
    "Download the training and validation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T23:44:27.539221844Z",
     "start_time": "2023-10-10T23:44:27.405299749Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_set: torch.utils.data.Dataset = torchvision.datasets.MNIST(\"./data\", train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "validation_set: torch.utils.data.Dataset = torchvision.datasets.MNIST(\"./data\", train=False, download=True, transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - MLP evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the example MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T23:44:27.553011701Z",
     "start_time": "2023-10-10T23:44:27.541645409Z"
    }
   },
   "outputs": [],
   "source": [
    "from bobnet import BobNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Create an instance of this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T23:44:27.562292617Z",
     "start_time": "2023-10-10T23:44:27.549221563Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model1 = BobNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Define the hyperparameters for this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T23:44:27.585806274Z",
     "start_time": "2023-10-10T23:44:27.563867031Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# batch size\n",
    "MLP_BATCH_SIZE=64\n",
    "\n",
    "# learning rate\n",
    "MLP_LEARNING_RATE=0.001\n",
    "\n",
    "# momentum\n",
    "MLP_MOMENTUM=0.9\n",
    "\n",
    "# training epochs to run\n",
    "MLP_EPOCHS=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Create the training and validation dataloaders from the datasets downloaded earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T23:44:27.611913299Z",
     "start_time": "2023-10-10T23:44:27.609459143Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create the training loader\n",
    "mlp_training_loader = DataLoader(training_set, batch_size=MLP_BATCH_SIZE, shuffle=True) \n",
    "\n",
    "# create the validation loader\n",
    "mlp_validation_loader = DataLoader(validation_set, batch_size=MLP_BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Define the loss function and the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T23:44:27.621901224Z",
     "start_time": "2023-10-10T23:44:27.609959279Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "mlp_optimizer = torch.optim.SGD(model1.parameters(), lr=MLP_LEARNING_RATE, momentum=MLP_MOMENTUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Run the training and validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T23:44:28.278065836Z",
     "start_time": "2023-10-10T23:44:27.610358765Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "# how many batches between logs\n",
    "LOGGING_INTERVAL=100\n",
    "\n",
    "utils.train_model(model1, MLP_EPOCHS, mlp_optimizer, mlp_loss_fn, mlp_training_loader, mlp_validation_loader, LOGGING_INTERVAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### QUESTIONS\n",
    "Explore the architecture on the script `mod1/bobnet.py`.\n",
    "1. Why does the input layer have 784 inputs? Consider the MNIST dataset samples' characteristics.\n",
    "Well since each image is 28x28, when flatten we wil endup with 784 (28*28) \n",
    "2. Why does the output layer have 10 outputs?\n",
    "The number of classes we want to identify are the numbers, that can be measured with 10 classes (0,1,2,3,4,5,6,7,8,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - CNN implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Head over to the `cnn.py` file and implement a convolutional architecture (add some convolutional layers and fully connected layers). You can search the LeNet architecture or AlexNet to get some insights and/or inspiration (you can implement a simpler version: with less layers). 2D convolutional layers in PyTorch are created using the `torch.nn.Conv2d` class. Activation and loss functions can be found under `torch.nn.functional` (like ReLU and softmax)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now, import the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-10T23:44:28.281022987Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cnn import CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Create an instance of this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T23:44:28.351537423Z",
     "start_time": "2023-10-10T23:44:28.329128600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model2 = CNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-10T23:44:28.329399351Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: run training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### QUESTIONS\n",
    "\n",
    "1. What are the advantages of using convolutional layers versus fully-connected layers for image processing?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
